{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f116ee-e5ec-4dcb-9b29-96734bb5bf89",
   "metadata": {},
   "source": [
    "# CNN LSTM\n",
    "\n",
    "This is the third model type of Jason Brownlee's book on LSTMs. CNNs were made to recognize and deal with pictures, so we can combine their feature extraction capabilities with the timecourse understanding of an LSTM to generate novel functionalities!!!\n",
    "\n",
    "Appropriate archetypes of problems to throw this network at are\n",
    "* Problems that have a 2D structure (Images) or 1D complex structure such as sentences, paragraphs, or a full document.\n",
    "\n",
    "* Problems that also have order to the input. Order of images such as a video, or words in a text. Also if the output we want has a temporal structure like coherent words.\n",
    "\n",
    "\n",
    "### Arcitecture\n",
    "CNN --> LSTM --> Dense output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d52d57ed-1a38-447a-b1db-058bb754c91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 21:31:19.353215: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-30 21:31:19.353255: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow Version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "# Neural net stuff\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "# number manip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random as rand\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# I also want to hide stinky warning boxes, and \n",
    "# this: https://stackoverflow.com/questions/9031783/hide-all-warnings-in-ipython\n",
    "# had some solutions.\n",
    "def hide_warnings():\n",
    "    from IPython.display import HTML\n",
    "    HTML('''<script>\n",
    "    var code_show_err = false; \n",
    "    var code_toggle_err = function() {\n",
    "     var stderrNodes = document.querySelectorAll('[data-mime-type=\"application/vnd.jupyter.stderr\"]')\n",
    "     var stderr = Array.from(stderrNodes)\n",
    "     if (code_show_err){\n",
    "         stderr.forEach(ele => ele.style.display = 'block');\n",
    "     } else {\n",
    "         stderr.forEach(ele => ele.style.display = 'none');\n",
    "     }\n",
    "     code_show_err = !code_show_err\n",
    "    } \n",
    "    document.addEventListener('DOMContentLoaded', code_toggle_err);\n",
    "    </script>\n",
    "    To toggle on/off output_stderr, click <a onclick=\"javascript:code_toggle_err()\">here</a>.''')\n",
    "    \n",
    "hide_warnings()\n",
    "###########################################################\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19006728-622c-4851-a80a-36d640eeece5",
   "metadata": {},
   "source": [
    "# CNN Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c22a77e-ad6f-49ed-88fd-eb1d2c04fcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 21:31:23.306154: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-01-30 21:31:23.308022: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-01-30 21:31:23.308033: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-01-30 21:31:23.308048: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-UJ90D1J): /proc/driver/nvidia/version does not exist\n",
      "2022-01-30 21:31:23.308695: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(1, (2,2), activation = 'relu', padding = 'same', input_shape=(10,10,1)))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27932bf-d8ae-44f0-9f0c-965ff264a321",
   "metadata": {},
   "source": [
    "One issue is that CNN model only outputs one \"timestep\"! We can achieve this by using the good ol' ```TimeDistributed``` layer. This actually acts as the bridge stiching the CNN (or any other non-LSTM type model) to the LSTM input.\n",
    "\n",
    "### Model definition\n",
    "It can be made in two ways\n",
    "1. First way\n",
    "```\n",
    "# define CNN model\n",
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(...))\n",
    "cnn.add(MaxPooling2D(...))\n",
    "cnn.add(Flatten())\n",
    "# define CNN LSTM model\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(cnn, ...))\n",
    "model.add(LSTM(..))\n",
    "model.add(Dense(...))\n",
    "```\n",
    "2. Second way\n",
    "```\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(...))\n",
    "model.add(TimeDistributed(MaxPooling2D(...)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(...))\n",
    "model.add(Dense(...))\n",
    "```\n",
    "\n",
    "I personally like the first way, although the book seems to enjoy the second method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226f6987-b5fb-4905-b733-95e55f378cb7",
   "metadata": {},
   "source": [
    "# Moving Square Problem\n",
    "A classification problem of a sequence of pixels moving either left, or right across the screen. The neural net should be able to say \"left or right\".\n",
    "\n",
    "This can be made is several steps:\n",
    "* Generate a square box (dim, dim)\n",
    "* Initialize a pixel as start, along one of the columns 50/50\n",
    "* Next step depends on previous step. Choose column over that is within the bounds of the box. (this feels recursive)\n",
    "\n",
    "### Book architecture\n",
    "The book chose to build this in two functions. The first one asks the user for the size of the square, and then initializes the starting frame. It then proceeds to call the next function which builds the \"next_frame\". The output of these two functions is a single \"film\".\n",
    "\n",
    "#### Structure\n",
    "Break problem into initialization and production. Wrap\n",
    "the production inside of initialization. Have users \n",
    "interact only with initialization side of things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fa3eef2-f429-4a41-9168-cdeeb5d1936c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[1., 0.],\n",
       "        [0., 0.]]),\n",
       " array([[1., 0.],\n",
       "        [0., 1.]])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_frames(size):\n",
    "    pass\n",
    "\n",
    "# I wanna make this fella recursive :)))\n",
    "def next_frame(last_step, last_frame, column):\n",
    "    # Prevents going out of bounds on the top\n",
    "    next_step_lower = max(0, last_step-1)\n",
    "    # Prevents going out of bounds on the bottom\n",
    "    next_step_upper = min(last_frame.shape[0]-1, last_step+1)\n",
    "    # Tells us what the next, legal, y-axis is\n",
    "    next_step = rand.randint(next_step_lower, next_step_upper)\n",
    "    # Sets the correct pixel to 1\n",
    "    next_frame = last_frame.copy()\n",
    "    next_frame[next_step, column] = 1\n",
    "    \n",
    "    return next_frame, next_step\n",
    "\n",
    "def build_frames(size):\n",
    "    frames = list()\n",
    "    frame = np.zeros((size, size))\n",
    "    step = rand.randint(0,size-1) # y-axis start\n",
    "    \n",
    "    right = 1 if rand.random() > 0.5 else 0\n",
    "    column = size-1 if right else 0 # X-axis start\n",
    "    frame[step, column] = 1\n",
    "    frames.append(frame)\n",
    "    \n",
    "    \n",
    "    for i in range(1,size):\n",
    "        column = size-i-1 if right else i\n",
    "        frame, step = next_frame(step, frame, column)\n",
    "        frames.append(frame)\n",
    "    \n",
    "    return frames, right\n",
    "\n",
    "frames, right = build_frames(2)\n",
    "print(right)\n",
    "frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c03dc2-ba76-4c79-b3ee-c7d869508ae9",
   "metadata": {},
   "source": [
    "# Moving square visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a273e323-4931-41fd-af8e-b122166654cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAAuCAYAAAAWRMPkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEDUlEQVR4nO3avy9rfQDH8c85Wq0irSISP8JgI5EICXs3CZOE1SAWk4U/wB9CIrEZRGwsQiTExGRRUaXiZxWl/T7Dk8iVy72ap+d8b577fo2np847yOc0HMcYIwCA/1zbAQDwt2KAAcASBhgALGGAAcASBhgALGGAAcCSQCknNzQ0mI6ODo9S/pXP53V2dqbr62tJkjHG8avj7e1NqVRKmUzmp9e87igUCkqn00qn0788z4+O8/NzXVxc+NJhjFEmk1EqlZLrumptbVVdXZ2KxaLnHcYY3dzc6PT0VG9vb+/Hg8GgLx13d3dKJpPK5/Ofvu5lRzabVTKZ1NPT0y+/nhcduVxOp6enymazv722Fx2O4/ynZ2/L1SGVOMAdHR3a29sr5S0lu76+1vLysnZ3d7W2tuZrx/39vVZWVrS5uakfn4/2oyOXy2l9fV1ra2sqFAqfnuNVR6FQ0NXVldLptB4eHrS9va2joyN99Yx4OTteX1+1s7Oj5eVlGWPU19ennp4ePT8/e95RKBS0v7+vpaUl3d7evh+PRCKedxSLRR0eHmpxcVGXl5efnuNVhzFGx8fHWlxcVDKZ/PQ9XnYkk0ktLCzo+Pj4t9f2oqO+vl5DQ0MlXduLDqnEAfZDbW2txsbGNDw8rIODA1+vHYlENDIyokQi8eGb6UdHKBRSIpHQwMDAlz9Irzry+bw2Nja0sLCgaDSq0dFRjY+Py3E+vWmXtSMQCKi3t1ednZ3KZrNaXV3V3Nyc4vG45x2u66qrq0uzs7Mfbnq5XM7zDtd11dnZqZmZmQ+fvn/kVYfjOGpra9P09PSXn7697GhqatLU1JReXl6+1VvujubmZs3Pz5d0bS86pD9wgIPBoOLxuOLxuCorK329diAQUCwWUywW+3Dcj46KigpFo1FFo9Evz/Gqo1gs6uTkRFtbW2pubtbExIRaWlrkup//i6CcHY7jqKamRjU1NXp8fHz/ROxHh+M4qq6uVnV19YfjfnVUVVWpqqrqy9e97AiHwwqHw986t9wdoVBIjY2N324td0dlZaVaW1tLvn65O6Q/cIDhv0AgoMHBQU1OTioWi6m9vf3LuzkddNBRvg4GGAoGg+rv71d3d7cqKioUDoet/GLTQcff1sEAQ67rKhKJKBKJ0EEHHT528BwwAFjCAAOAJQwwAFjCAAOAJQwwAFjCAAOAJQwwAFjCAAOAJQwwAFjCAAOAJQwwAFjCAAOAJQwwAFjCAAOAJQwwAFjCAAOAJQwwAFjCAAOAJQwwAFjCAAOAJQwwAFjiGGO+f7LjZCSdeJfzk3ZjTCMddNBBx/+tQypxgAEA5cOfIADAEgYYACxhgAHAEgYYACxhgAHAEgYYACxhgAHAEgYYACxhgAHAkn8A1SqeoE0bxdcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# generate sequence of frames\n",
    "size = 10\n",
    "frames, right = build_frames(size)\n",
    "# plot all feames\n",
    "plt.figure()\n",
    "for i in range(size):\n",
    "# create a grayscale subplot for each frame\n",
    "    plt.subplot(1, size, i+1)\n",
    "    plt.imshow(frames[i], cmap= \"Greys\" )\n",
    "    # turn of the scale to make it cleaer\n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "# show the plot\n",
    "plt.show()\n",
    "print(right)\n",
    "\n",
    "def viz_moving_box(seq):\n",
    "    size = seq.shape[0]\n",
    "    plt.figure()\n",
    "    for i in range(size):\n",
    "    # create a grayscale subplot for each frame\n",
    "        plt.subplot(1, size, i+1)\n",
    "        plt.imshow(seq[i], cmap= \"Greys\" )\n",
    "        # turn of the scale to make it cleaer\n",
    "        ax = plt.gca()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    # show the plot\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e27eed-0fdd-4a38-81f9-32176b771e8e",
   "metadata": {},
   "source": [
    "# Reshaping for input to CNN\n",
    "Quite a bit goes into this. Normally it would be re-shaped to ```[width, height, channels]``` but in our case we need ```[samples,timesteps,width,height,channels]```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc2defe-da5c-4196-8d5d-4de58dd080d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(size, n_patterns):\n",
    "    X,y = list(), list()\n",
    "    for _ in range(n_patterns):\n",
    "        frames,right = build_frames(size)\n",
    "        X.append(frames)\n",
    "        y.append(right)\n",
    "    X = np.array(X).reshape(n_patterns, size, size, size, 1)\n",
    "    y = np.array(y).reshape(n_patterns, 1)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62904dcb-5eef-4591-8c05-8e19a7db22ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 5\n",
    "n_patterns = 10\n",
    "X,y = generate_examples(5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fffe349-868e-4e5d-8fee-bb6b831f9ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(n_patterns*size,size,size,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cfef76-4cfc-49ca-a243-cd26065126af",
   "metadata": {},
   "source": [
    "Apparently this is a binary classifiation problem, right or left. This book uses the **sigmoid activation function** and the **binary_crossentropy** loss function. \n",
    "\n",
    "The Conv2D input layer acts as feature extraction, and there is some weird pooling layer happeining that I have never heard of.\n",
    "\n",
    "# Defintion of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "5e1bd2a0-b6d7-4ea6-950a-fff74bd1cfe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 1s 3ms/step - loss: 0.6174 - acc: 0.7047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c82366e50>"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size=10\n",
    "\n",
    "# CNN feature extraction\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(2,(2,2), activation='relu'), input_shape=(None,size,size,1)))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2,2))))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "# LSTM\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "#print(model.summary())\n",
    "X,y = generate_examples(size, 150)\n",
    "model.fit(X,y,batch_size=2, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "6d304792-9e2a-479c-b98c-60072fb983fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 1s 4ms/step - loss: 0.6751 - acc: 0.5738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9c8a0a8460>"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y = generate_examples(size, 150)\n",
    "model.fit(X,y,batch_size=5, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169e5d23-a713-4866-aaed-9bfd9a44e1e7",
   "metadata": {},
   "source": [
    "# Evaluation and Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "dbe75f54-57f4-43c9-98b8-40d47872c673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.09634286910295486, acc: 0.9857000112533569\n"
     ]
    }
   ],
   "source": [
    "X_eval,y_eval = generate_examples(size,10000)\n",
    "loss, acc = model.evaluate(X_eval, y_eval, verbose=0)\n",
    "print(f'Loss: {loss}, acc: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "50b4977f-e4d9-41b5-bf0a-52dbf0b7fba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAAuCAYAAAAWRMPkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAFKElEQVR4nO3dsUsbfQDG8eeXprnclbNq76JNBROpFGobBI2oIKngIFJoNqdS6iYOGVo3/wjp0qFL6V/QqZTiqIMgzg4RfBWURGkwCSYak7xTQl59ozlzl5+2zwccWu/kCw3P6Xmkolwug4iIWs8lO4CI6G/FASYikoQDTEQkCQeYiEgSDjARkSQcYCIiSdxWDjYMoxwIBBxKuWxnZwdHR0fiLnVks1ns7u4in8+jq6sL3d3duHfvHjvYwQ52XGJpgH0+H758+YLOzk50d3dDURRbAusZHh6+sqNCCAHDMODz+eDxeKR1AEA8Hsfa2hrS6TRev36N6elpaJomreP4+BjPnz/H2NgYdF3H48eP8ejRo6ZeXOxgBzua7wAsDvDe3h5isRimpqYwPz8P0zRvHNWMSkeFEAJv3rzB3NycIwNspWNiYgLv37+HYRjQdd3xi9R1HR0dHVhfX8enT5+QyWTw7t07RKNR267u7GAHO27O0gBns1msra0hGAzi7OzMkaBGO1ZXV6t/drlcCIVCKBQKt6LD7/e39OJ0VUdnZyd0Xcfm5iYSiQQmJydRKpWkd7x69QrFYhHlchlC/O9PZy3vqGVHkx0ddjTZ3XHTpr+to5FzLA2waZqYnZ1FOBzGgwcPrJxqq0pHhRACkUgEqqqy40KHEAKBQABv375FOp3G6OioY1fzRjtSqRQ0TcP6+jo0TUMgEIBpmrZ13aTj/v371eMVRUEwGIRhGE0NcbMdtTRNQ19fH9rb2y032dlRS9d1BINBPHz4kB0XOnK5HPb29pBIJJDNZuueI6y8F8Tg4GB5ZWUFiqJAVVW43Zb227Lh4WFsbGxcerVVOmp5vV6oqurIuNz1jnw+j1wuh1KpBFVV4fV64XLd/AGYZjvy+Tx+/fqFb9++oVQqIRaLYWZmxvLtIzs7Dg8Pq8c+efIEHz9+RCQSaeg17lRHrf7+fiwuLmJkZKTuv10rOmqFQiEsLi4iFAr956LAjhASiQSWl5fx48cPxONxnJycNP9LOLfbDcMwrJziCHZY6/B6vfB6vbemo1AoQFEUxONx5PN5HB0dNfwjnlMdBwcH1c+n02n8/v1beket8/PzK7+TalVHLU3TkMvlpHeoqopMJoNSqQQhRPVDZsfp6Sn29/extbWF8/Pz+i3XflUim7lcLrx8+RLz8/MoFAoYGhpy7LZIox3Hx8fVv29vb8fAwIDt96atdtQyTROBQMDRpkY6avX09MDv99veZLVDVVVsb28jk8nAMAw8e/YMbW1tUjvS6TRevHiBDx8+4OvXr3XP4QBTy7lcLjx9+hQLCwsAAI/HI22AKx2134ELIaAoSsua6nVcPMbj8Tg+wNd1XDzeiad8rHYkk0l8/vwZ379/x/j4OJaWlmwb4Jt2hMNhxGIxDAwM4OfPn3XP4QBTy1UGrhWP6LHjz+84PT1FsVhEMplEKpWy7WmoZjqy2Sx0XUdbW9uVF3IOMBHdaZqmIRqNwu/3o7e3F11dXXemgwNMRHeaqqoYGxurPiHSyCNjt6WDA0xEd5pT96Jb0cF3QyMikoQDTEQkCQeYiEgSDjARkSQcYCIiSTjARESScICJiCThABMRScIBJiKShANMRCQJB5iISBIOMBGRJBxgIiJJOMBERJJwgImIJLH039ILIQ4B/ONcziW95XLZZAc72MGOP60DsDjARERkH96CICKShANMRCQJB5iISBIOMBGRJBxgIiJJOMBERJJwgImIJOEAExFJwgEmIpLkX9k8QNl5QfQOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 0\n",
      "Predicted: 0.10113298892974854\n"
     ]
    }
   ],
   "source": [
    "X_viz, y_viz = generate_examples(size,1)\n",
    "y_hat = model.predict(X_viz)\n",
    "viz_moving_box(X_viz.reshape(size,size,size))\n",
    "print(f'Actual: {y_viz.reshape(1)[0]}\\nPredicted: {y_hat.reshape(1)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc65f2b-ab80-44ed-8f53-317d98bce34d",
   "metadata": {},
   "source": [
    "---\n",
    "It is very interesting to me that when I have reduced training examples to only 300 from recommended 50000, I am indeed getting shittier loss and accuracy for training, yet upon evaluation I am getting some stellar resultes! What the heck is happening!\n",
    "\n",
    "I got down to 200 training examples, and truly my loss is doing very poorly ```Loss: 0.4867803454399109```, and yet my accuracy is still very high!  ```acc: 0.9206500053405762```. This is on 100,000 evaluation generated examples, so this isn't a fluke! I would be fairly impressed with this model tbh.\n",
    "\n",
    "Why does it do so well on the eval!!! It does spectacularly!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "63f89d08-9b4a-4931-9f90-f9d62c079442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(f'./Accidently_made_very_good_CNN_LSTM_150-training-examples_loss-0.08_acc-0.98')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
